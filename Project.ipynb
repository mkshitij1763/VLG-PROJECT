{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsGSFFLjrmL-",
        "outputId": "41b42cd5-5018-41e3-fff0-fea97ea23d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pq9CAEw1rg-5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Input,Conv2D,Concatenate\n",
        "from keras.models import Model\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wvc0_VfRrg-6"
      },
      "outputs": [],
      "source": [
        "TARGET_SIZE = 256\n",
        "BATCH_SIZE = 32\n",
        "MAX_TRAIN_IMAGES = 400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9JzD3Iorg-7"
      },
      "source": [
        "PRE_REQUISITE FUNCTION TO GENERATE DATA FROM FOLDERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5KIhW-7irg-7"
      },
      "outputs": [],
      "source": [
        "def load_image(file_path):\n",
        "    image_data = tf.io.read_file(file_path)\n",
        "    decoded_image = tf.image.decode_png(image_data, channels=3)\n",
        "    resized_image = tf.image.resize(images=decoded_image, size=[TARGET_SIZE, TARGET_SIZE])\n",
        "    normalized_image = resized_image / 255.0  # Scale pixel values to [0, 1]\n",
        "    return normalized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JSThutVSrg-7"
      },
      "outputs": [],
      "source": [
        "def image_data_generator(image_paths):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kf7WB-Prg-7"
      },
      "source": [
        "DATASET PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rCUoqMZprg-8"
      },
      "outputs": [],
      "source": [
        "# Paths to your datasets\n",
        "train_low_light_image_paths = sorted(glob(\"/content/drive/MyDrive/lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
        "val_low_light_image_paths = sorted(glob(\"/content/drive/MyDrive/lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
        "\n",
        "test_low_light_image_paths = sorted(glob(\"/content/drive/MyDrive/Train/low/*\"))\n",
        "test_high_light_image_paths = sorted(glob(\"/content/drive/MyDrive/Train/high/*\"))\n",
        "\n",
        "# Generate datasetsc\n",
        "train_dataset = image_data_generator(train_low_light_image_paths)\n",
        "val_dataset = image_data_generator(val_low_light_image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IeYvhYomrg-8"
      },
      "outputs": [],
      "source": [
        "def build_dce_net():\n",
        "    input_image = Input(shape=[None, None, 3])\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(input_image)\n",
        "    conv2 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(conv1)\n",
        "    conv3 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(conv2)\n",
        "    conv4 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(conv3)\n",
        "\n",
        "    concat1 = Concatenate(axis=-1)([conv4, conv3])\n",
        "    conv5 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(concat1)\n",
        "\n",
        "    concat2 = Concatenate(axis=-1)([conv5, conv2])\n",
        "    conv6 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(concat2)\n",
        "\n",
        "    concat3 = Concatenate(axis=-1)([conv6, conv1])\n",
        "    output_image = Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(concat3)\n",
        "\n",
        "    return Model(inputs=input_image, outputs=output_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVcXP0Efrg-8"
      },
      "source": [
        "CUSTOM LOSS FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m5Lq6xsbrg-8"
      },
      "outputs": [],
      "source": [
        "def compute_color_constancy_loss(image_batch):\n",
        "    # Calculate the mean of each RGB channel\n",
        "    mean_rgb_values = tf.reduce_mean(image_batch, axis=(1, 2), keepdims=True)\n",
        "    mean_r_channel = mean_rgb_values[:, :, :, 0]\n",
        "    mean_g_channel = mean_rgb_values[:, :, :, 1]\n",
        "    mean_b_channel = mean_rgb_values[:, :, :, 2]\n",
        "\n",
        "    # Compute the squared differences between the channel means\n",
        "    diff_red_green = tf.square(mean_r_channel - mean_g_channel)\n",
        "    diff_red_blue = tf.square(mean_r_channel - mean_b_channel)\n",
        "    diff_green_blue = tf.square(mean_g_channel - mean_b_channel)\n",
        "\n",
        "    # Calculate the color constancy loss\n",
        "    color_loss = tf.sqrt(diff_red_green + diff_red_blue + diff_green_blue)\n",
        "    return color_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xKREcF4drg-8"
      },
      "outputs": [],
      "source": [
        "def compute_exposure_loss(image, target_exposure=0.6):\n",
        "    # Calculate the mean across the RGB channels\n",
        "    grayscale_image = tf.reduce_mean(image, axis=3, keepdims=True)\n",
        "\n",
        "    # Pool the image using a 16x16 kernel with non-overlapping regions\n",
        "    pooled_mean = tf.nn.avg_pool2d(grayscale_image, ksize=16, strides=16, padding=\"VALID\")\n",
        "\n",
        "    # Calculate the exposure loss\n",
        "    exposure_loss_value = tf.reduce_mean(tf.square(pooled_mean - target_exposure))\n",
        "\n",
        "    return exposure_loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bq-Y7VZUrg-8"
      },
      "outputs": [],
      "source": [
        "def illumination_smoothness_loss(image):\n",
        "    # Get the dimensions of the input tensor\n",
        "    batch_size = tf.shape(image)[0]\n",
        "    height = tf.shape(image)[1]\n",
        "    width = tf.shape(image)[2]\n",
        "    channels = tf.shape(image)[3]\n",
        "\n",
        "    # Calculate the total number of horizontal and vertical differences\n",
        "    horizontal_count = (width - 1) * channels\n",
        "    vertical_count = width * (channels - 1)\n",
        "\n",
        "    # Compute the horizontal and vertical total variation losses\n",
        "    horizontal_tv_loss = tf.reduce_sum(tf.square(image[:, 1:, :, :] - image[:, :height - 1, :, :]))\n",
        "    vertical_tv_loss = tf.reduce_sum(tf.square(image[:, :, 1:, :] - image[:, :, :width - 1, :]))\n",
        "\n",
        "    # Convert counts and batch size to float for division\n",
        "    batch_size = tf.cast(batch_size, dtype=tf.float32)\n",
        "    horizontal_count = tf.cast(horizontal_count, dtype=tf.float32)\n",
        "    vertical_count = tf.cast(vertical_count, dtype=tf.float32)\n",
        "\n",
        "    # Calculate the smoothness loss\n",
        "    smoothness_loss = 2 * (horizontal_tv_loss / horizontal_count + vertical_tv_loss / vertical_count) / batch_size\n",
        "\n",
        "    return smoothness_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqvwEImzrg-9"
      },
      "source": [
        "CUSTOM ZERO DCE MODEL ----> MODIFYING IT'S Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jTYC9baHrg-9"
      },
      "outputs": [],
      "source": [
        "class ZeroDCE(Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dce_model = build_dce_net()\n",
        "\n",
        "    def compile(self, learning_rate, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(name=\"illumination_smoothness_loss\")\n",
        "        self.color_constancy_loss_tracker = keras.metrics.Mean(name=\"color_constancy_loss\")\n",
        "        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.illumination_smoothness_loss_tracker,\n",
        "            self.color_constancy_loss_tracker,\n",
        "            self.exposure_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def get_enhanced_image(self, data, output):\n",
        "        r_layers = [output[:, :, :, i:i+3] for i in range(0, 24, 3)]\n",
        "        x = data\n",
        "        for r in r_layers:\n",
        "            x = x + r * (tf.square(x) - x)\n",
        "        return x\n",
        "\n",
        "    def call(self, data):\n",
        "        dce_net_output = self.dce_model(data)\n",
        "        return self.get_enhanced_image(data, dce_net_output)\n",
        "\n",
        "    def compute_losses(self, data, output):\n",
        "        enhanced_image = self.get_enhanced_image(data, output)\n",
        "        loss_illumination = 200 * illumination_smoothness_loss(output)\n",
        "        loss_color_constancy = 5 * tf.reduce_mean(compute_color_constancy_loss(enhanced_image))\n",
        "        loss_exposure = 10 * tf.reduce_mean(compute_exposure_loss(enhanced_image))\n",
        "        total_loss = loss_illumination + loss_color_constancy + loss_exposure\n",
        "\n",
        "        return {\n",
        "            \"total_loss\": total_loss,\n",
        "            \"illumination_smoothness_loss\": loss_illumination,\n",
        "            \"color_constancy_loss\": loss_color_constancy,\n",
        "            \"exposure_loss\": loss_exposure,\n",
        "        }\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self.dce_model(data)\n",
        "            losses = self.compute_losses(data, output)\n",
        "\n",
        "        gradients = tape.gradient(losses[\"total_loss\"], self.dce_model.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n",
        "\n",
        "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
        "        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n",
        "        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n",
        "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
        "\n",
        "        return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        output = self.dce_model(data)\n",
        "        losses = self.compute_losses(data, output)\n",
        "\n",
        "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
        "        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n",
        "        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n",
        "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
        "\n",
        "        return {metric.name: metric.result() for metric in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uDnP9yJdrg-9"
      },
      "outputs": [],
      "source": [
        "def plot(images, titles, figure_size=(10, 10)):\n",
        "    # Ensure images and titles are lists\n",
        "    if not isinstance(images, list):\n",
        "        images = [images]\n",
        "    if not isinstance(titles, list):\n",
        "        titles = [titles]\n",
        "\n",
        "    fig = plt.figure(figsize=figure_size)\n",
        "    for i in range(len(images)):\n",
        "        ax = fig.add_subplot(1, len(images), i + 1)\n",
        "        ax.set_title(titles[i])\n",
        "        if images[i].ndim == 2:  # Grayscale image\n",
        "            plt.imshow(images[i], cmap='gray')\n",
        "        else:  # RGB image\n",
        "            plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MvalysA3rg-9"
      },
      "outputs": [],
      "source": [
        "def calculate_psnr(original_image, enhanced_image):\n",
        "    # Convert the images to numpy arrays if they are not already.\n",
        "    original_image = np.asarray(original_image)\n",
        "    enhanced_image = np.asarray(enhanced_image)\n",
        "\n",
        "    # Ensure the images have the same shape.\n",
        "    if original_image.shape != enhanced_image.shape:\n",
        "        raise ValueError(\"The shapes of the input images must be the same.\")\n",
        "\n",
        "    # Calculate the mean squared error (MSE) between the two images.\n",
        "    mse = np.mean(np.square(original_image - enhanced_image))\n",
        "\n",
        "    # If MSE is close to zero, return a high PSNR (infinity in theory).\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    # Calculate the peak signal-to-noise ratio (PSNR).\n",
        "    max_pixel_value = 255.0  # Assuming pixel values range from 0 to 255.\n",
        "    psnr = 10 * np.log10((max_pixel_value ** 2) / mse)\n",
        "\n",
        "    return psnr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgXB7-j9rg-9",
        "outputId": "18e3a433-db39-47e9-92e5-a97e46415724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - 165s 13s/step - total_loss: 5.6860 - illumination_smoothness_loss: 2.6345 - color_constancy_loss: 0.1187 - exposure_loss: 2.9329 - val_total_loss: 6.8089 - val_illumination_smoothness_loss: 3.7706 - val_color_constancy_loss: 0.1134 - val_exposure_loss: 2.9250\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - 8s 655ms/step - total_loss: 5.0390 - illumination_smoothness_loss: 1.9926 - color_constancy_loss: 0.1121 - exposure_loss: 2.9343 - val_total_loss: 6.0076 - val_illumination_smoothness_loss: 2.9692 - val_color_constancy_loss: 0.1131 - val_exposure_loss: 2.9253\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - 8s 663ms/step - total_loss: 4.6091 - illumination_smoothness_loss: 1.5666 - color_constancy_loss: 0.1076 - exposure_loss: 2.9349 - val_total_loss: 5.4278 - val_illumination_smoothness_loss: 2.3895 - val_color_constancy_loss: 0.1138 - val_exposure_loss: 2.9245\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - 8s 633ms/step - total_loss: 4.2782 - illumination_smoothness_loss: 1.2396 - color_constancy_loss: 0.1046 - exposure_loss: 2.9340 - val_total_loss: 4.9683 - val_illumination_smoothness_loss: 1.9300 - val_color_constancy_loss: 0.1163 - val_exposure_loss: 2.9220\n",
            "Epoch 5/50\n",
            "12/12 [==============================] - 8s 661ms/step - total_loss: 4.0221 - illumination_smoothness_loss: 0.9870 - color_constancy_loss: 0.1032 - exposure_loss: 2.9319 - val_total_loss: 4.6248 - val_illumination_smoothness_loss: 1.5879 - val_color_constancy_loss: 0.1181 - val_exposure_loss: 2.9188\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - 8s 637ms/step - total_loss: 3.8377 - illumination_smoothness_loss: 0.8066 - color_constancy_loss: 0.1031 - exposure_loss: 2.9280 - val_total_loss: 4.3691 - val_illumination_smoothness_loss: 1.3359 - val_color_constancy_loss: 0.1195 - val_exposure_loss: 2.9137\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - 8s 638ms/step - total_loss: 3.7024 - illumination_smoothness_loss: 0.6757 - color_constancy_loss: 0.1036 - exposure_loss: 2.9231 - val_total_loss: 4.1741 - val_illumination_smoothness_loss: 1.1453 - val_color_constancy_loss: 0.1201 - val_exposure_loss: 2.9087\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - 8s 641ms/step - total_loss: 3.6002 - illumination_smoothness_loss: 0.5775 - color_constancy_loss: 0.1043 - exposure_loss: 2.9185 - val_total_loss: 4.0217 - val_illumination_smoothness_loss: 0.9969 - val_color_constancy_loss: 0.1205 - val_exposure_loss: 2.9042\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - 7s 616ms/step - total_loss: 3.5209 - illumination_smoothness_loss: 0.5016 - color_constancy_loss: 0.1049 - exposure_loss: 2.9144 - val_total_loss: 3.8996 - val_illumination_smoothness_loss: 0.8785 - val_color_constancy_loss: 0.1213 - val_exposure_loss: 2.8997\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - 7s 579ms/step - total_loss: 3.4572 - illumination_smoothness_loss: 0.4415 - color_constancy_loss: 0.1056 - exposure_loss: 2.9101 - val_total_loss: 3.7994 - val_illumination_smoothness_loss: 0.7823 - val_color_constancy_loss: 0.1219 - val_exposure_loss: 2.8952\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - 8s 637ms/step - total_loss: 3.4049 - illumination_smoothness_loss: 0.3928 - color_constancy_loss: 0.1063 - exposure_loss: 2.9057 - val_total_loss: 3.7161 - val_illumination_smoothness_loss: 0.7030 - val_color_constancy_loss: 0.1228 - val_exposure_loss: 2.8903\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - 9s 753ms/step - total_loss: 3.3609 - illumination_smoothness_loss: 0.3529 - color_constancy_loss: 0.1071 - exposure_loss: 2.9009 - val_total_loss: 3.6456 - val_illumination_smoothness_loss: 0.6367 - val_color_constancy_loss: 0.1238 - val_exposure_loss: 2.8851\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - 8s 664ms/step - total_loss: 3.3230 - illumination_smoothness_loss: 0.3193 - color_constancy_loss: 0.1079 - exposure_loss: 2.8959 - val_total_loss: 3.5847 - val_illumination_smoothness_loss: 0.5802 - val_color_constancy_loss: 0.1248 - val_exposure_loss: 2.8797\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - 7s 612ms/step - total_loss: 3.2899 - illumination_smoothness_loss: 0.2905 - color_constancy_loss: 0.1087 - exposure_loss: 2.8906 - val_total_loss: 3.5318 - val_illumination_smoothness_loss: 0.5319 - val_color_constancy_loss: 0.1259 - val_exposure_loss: 2.8740\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - 8s 616ms/step - total_loss: 3.2606 - illumination_smoothness_loss: 0.2659 - color_constancy_loss: 0.1096 - exposure_loss: 2.8851 - val_total_loss: 3.4852 - val_illumination_smoothness_loss: 0.4902 - val_color_constancy_loss: 0.1270 - val_exposure_loss: 2.8680\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - 7s 614ms/step - total_loss: 3.2345 - illumination_smoothness_loss: 0.2447 - color_constancy_loss: 0.1104 - exposure_loss: 2.8794 - val_total_loss: 3.4438 - val_illumination_smoothness_loss: 0.4539 - val_color_constancy_loss: 0.1281 - val_exposure_loss: 2.8619\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - 8s 605ms/step - total_loss: 3.2111 - illumination_smoothness_loss: 0.2262 - color_constancy_loss: 0.1113 - exposure_loss: 2.8736 - val_total_loss: 3.4068 - val_illumination_smoothness_loss: 0.4220 - val_color_constancy_loss: 0.1292 - val_exposure_loss: 2.8556\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - 8s 669ms/step - total_loss: 3.1896 - illumination_smoothness_loss: 0.2099 - color_constancy_loss: 0.1122 - exposure_loss: 2.8675 - val_total_loss: 3.3730 - val_illumination_smoothness_loss: 0.3936 - val_color_constancy_loss: 0.1304 - val_exposure_loss: 2.8491\n",
            "Epoch 19/50\n",
            "12/12 [==============================] - 7s 609ms/step - total_loss: 3.1698 - illumination_smoothness_loss: 0.1955 - color_constancy_loss: 0.1131 - exposure_loss: 2.8611 - val_total_loss: 3.3420 - val_illumination_smoothness_loss: 0.3684 - val_color_constancy_loss: 0.1316 - val_exposure_loss: 2.8420\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - 8s 604ms/step - total_loss: 3.1512 - illumination_smoothness_loss: 0.1829 - color_constancy_loss: 0.1141 - exposure_loss: 2.8542 - val_total_loss: 3.3133 - val_illumination_smoothness_loss: 0.3460 - val_color_constancy_loss: 0.1330 - val_exposure_loss: 2.8343\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - 8s 628ms/step - total_loss: 3.1336 - illumination_smoothness_loss: 0.1718 - color_constancy_loss: 0.1152 - exposure_loss: 2.8466 - val_total_loss: 3.2864 - val_illumination_smoothness_loss: 0.3260 - val_color_constancy_loss: 0.1346 - val_exposure_loss: 2.8259\n",
            "Epoch 22/50\n",
            "12/12 [==============================] - 7s 602ms/step - total_loss: 3.1169 - illumination_smoothness_loss: 0.1621 - color_constancy_loss: 0.1163 - exposure_loss: 2.8385 - val_total_loss: 3.2613 - val_illumination_smoothness_loss: 0.3081 - val_color_constancy_loss: 0.1363 - val_exposure_loss: 2.8169\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - 8s 698ms/step - total_loss: 3.1008 - illumination_smoothness_loss: 0.1535 - color_constancy_loss: 0.1176 - exposure_loss: 2.8297 - val_total_loss: 3.2374 - val_illumination_smoothness_loss: 0.2920 - val_color_constancy_loss: 0.1382 - val_exposure_loss: 2.8072\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - 8s 667ms/step - total_loss: 3.0851 - illumination_smoothness_loss: 0.1459 - color_constancy_loss: 0.1190 - exposure_loss: 2.8202 - val_total_loss: 3.2145 - val_illumination_smoothness_loss: 0.2777 - val_color_constancy_loss: 0.1403 - val_exposure_loss: 2.7966\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - 8s 607ms/step - total_loss: 3.0695 - illumination_smoothness_loss: 0.1392 - color_constancy_loss: 0.1206 - exposure_loss: 2.8097 - val_total_loss: 3.1922 - val_illumination_smoothness_loss: 0.2650 - val_color_constancy_loss: 0.1427 - val_exposure_loss: 2.7845\n",
            "Epoch 26/50\n",
            "12/12 [==============================] - 8s 622ms/step - total_loss: 3.0536 - illumination_smoothness_loss: 0.1337 - color_constancy_loss: 0.1224 - exposure_loss: 2.7976 - val_total_loss: 3.1700 - val_illumination_smoothness_loss: 0.2540 - val_color_constancy_loss: 0.1455 - val_exposure_loss: 2.7705\n",
            "Epoch 27/50\n",
            "12/12 [==============================] - 7s 607ms/step - total_loss: 3.0371 - illumination_smoothness_loss: 0.1290 - color_constancy_loss: 0.1246 - exposure_loss: 2.7835 - val_total_loss: 3.1473 - val_illumination_smoothness_loss: 0.2445 - val_color_constancy_loss: 0.1489 - val_exposure_loss: 2.7539\n",
            "Epoch 28/50\n",
            "12/12 [==============================] - 8s 605ms/step - total_loss: 3.0192 - illumination_smoothness_loss: 0.1256 - color_constancy_loss: 0.1273 - exposure_loss: 2.7663 - val_total_loss: 3.1232 - val_illumination_smoothness_loss: 0.2370 - val_color_constancy_loss: 0.1530 - val_exposure_loss: 2.7332\n",
            "Epoch 29/50\n",
            "12/12 [==============================] - 8s 621ms/step - total_loss: 2.9988 - illumination_smoothness_loss: 0.1237 - color_constancy_loss: 0.1307 - exposure_loss: 2.7444 - val_total_loss: 3.0962 - val_illumination_smoothness_loss: 0.2320 - val_color_constancy_loss: 0.1584 - val_exposure_loss: 2.7057\n",
            "Epoch 30/50\n",
            "12/12 [==============================] - 8s 626ms/step - total_loss: 2.9741 - illumination_smoothness_loss: 0.1239 - color_constancy_loss: 0.1355 - exposure_loss: 2.7147 - val_total_loss: 3.0632 - val_illumination_smoothness_loss: 0.2301 - val_color_constancy_loss: 0.1662 - val_exposure_loss: 2.6669\n",
            "Epoch 31/50\n",
            "12/12 [==============================] - 8s 608ms/step - total_loss: 2.9405 - illumination_smoothness_loss: 0.1271 - color_constancy_loss: 0.1426 - exposure_loss: 2.6707 - val_total_loss: 3.0175 - val_illumination_smoothness_loss: 0.2341 - val_color_constancy_loss: 0.1788 - val_exposure_loss: 2.6047\n",
            "Epoch 32/50\n",
            "12/12 [==============================] - 7s 608ms/step - total_loss: 2.8883 - illumination_smoothness_loss: 0.1357 - color_constancy_loss: 0.1547 - exposure_loss: 2.5979 - val_total_loss: 2.9441 - val_illumination_smoothness_loss: 0.2501 - val_color_constancy_loss: 0.2010 - val_exposure_loss: 2.4931\n",
            "Epoch 33/50\n",
            "12/12 [==============================] - 7s 605ms/step - total_loss: 2.7952 - illumination_smoothness_loss: 0.1545 - color_constancy_loss: 0.1775 - exposure_loss: 2.4632 - val_total_loss: 2.8080 - val_illumination_smoothness_loss: 0.2871 - val_color_constancy_loss: 0.2425 - val_exposure_loss: 2.2784\n",
            "Epoch 34/50\n",
            "12/12 [==============================] - 8s 605ms/step - total_loss: 2.6177 - illumination_smoothness_loss: 0.1901 - color_constancy_loss: 0.2239 - exposure_loss: 2.2037 - val_total_loss: 2.5371 - val_illumination_smoothness_loss: 0.3240 - val_color_constancy_loss: 0.3218 - val_exposure_loss: 1.8913\n",
            "Epoch 35/50\n",
            "12/12 [==============================] - 7s 610ms/step - total_loss: 2.2972 - illumination_smoothness_loss: 0.2348 - color_constancy_loss: 0.3162 - exposure_loss: 1.7461 - val_total_loss: 2.0933 - val_illumination_smoothness_loss: 0.3446 - val_color_constancy_loss: 0.4620 - val_exposure_loss: 1.2867\n",
            "Epoch 36/50\n",
            "12/12 [==============================] - 8s 661ms/step - total_loss: 1.9019 - illumination_smoothness_loss: 0.2785 - color_constancy_loss: 0.4749 - exposure_loss: 1.1484 - val_total_loss: 1.7033 - val_illumination_smoothness_loss: 0.3710 - val_color_constancy_loss: 0.6426 - val_exposure_loss: 0.6897\n",
            "Epoch 37/50\n",
            "12/12 [==============================] - 7s 602ms/step - total_loss: 1.6925 - illumination_smoothness_loss: 0.2981 - color_constancy_loss: 0.6141 - exposure_loss: 0.7803 - val_total_loss: 1.5934 - val_illumination_smoothness_loss: 0.3508 - val_color_constancy_loss: 0.7292 - val_exposure_loss: 0.5134\n",
            "Epoch 38/50\n",
            "12/12 [==============================] - 7s 605ms/step - total_loss: 1.6215 - illumination_smoothness_loss: 0.2801 - color_constancy_loss: 0.6476 - exposure_loss: 0.6939 - val_total_loss: 1.5651 - val_illumination_smoothness_loss: 0.3224 - val_color_constancy_loss: 0.7469 - val_exposure_loss: 0.4958\n",
            "Epoch 39/50\n",
            "12/12 [==============================] - 8s 664ms/step - total_loss: 1.5865 - illumination_smoothness_loss: 0.2546 - color_constancy_loss: 0.6432 - exposure_loss: 0.6886 - val_total_loss: 1.5426 - val_illumination_smoothness_loss: 0.3051 - val_color_constancy_loss: 0.7394 - val_exposure_loss: 0.4981\n",
            "Epoch 40/50\n",
            "12/12 [==============================] - 8s 605ms/step - total_loss: 1.5736 - illumination_smoothness_loss: 0.2379 - color_constancy_loss: 0.6390 - exposure_loss: 0.6966 - val_total_loss: 1.5228 - val_illumination_smoothness_loss: 0.2857 - val_color_constancy_loss: 0.7277 - val_exposure_loss: 0.5094\n",
            "Epoch 41/50\n",
            "12/12 [==============================] - 7s 609ms/step - total_loss: 1.5604 - illumination_smoothness_loss: 0.2245 - color_constancy_loss: 0.6383 - exposure_loss: 0.6976 - val_total_loss: 1.5058 - val_illumination_smoothness_loss: 0.2736 - val_color_constancy_loss: 0.7209 - val_exposure_loss: 0.5112\n",
            "Epoch 42/50\n",
            "12/12 [==============================] - 8s 663ms/step - total_loss: 1.5457 - illumination_smoothness_loss: 0.2102 - color_constancy_loss: 0.6411 - exposure_loss: 0.6944 - val_total_loss: 1.4893 - val_illumination_smoothness_loss: 0.2592 - val_color_constancy_loss: 0.7265 - val_exposure_loss: 0.5036\n",
            "Epoch 43/50\n",
            "12/12 [==============================] - 8s 605ms/step - total_loss: 1.5347 - illumination_smoothness_loss: 0.2013 - color_constancy_loss: 0.6439 - exposure_loss: 0.6896 - val_total_loss: 1.4827 - val_illumination_smoothness_loss: 0.2481 - val_color_constancy_loss: 0.7356 - val_exposure_loss: 0.4990\n",
            "Epoch 44/50\n",
            "12/12 [==============================] - 7s 603ms/step - total_loss: 1.5235 - illumination_smoothness_loss: 0.1946 - color_constancy_loss: 0.6425 - exposure_loss: 0.6865 - val_total_loss: 1.4778 - val_illumination_smoothness_loss: 0.2406 - val_color_constancy_loss: 0.7403 - val_exposure_loss: 0.4969\n",
            "Epoch 45/50\n",
            "12/12 [==============================] - 8s 625ms/step - total_loss: 1.5147 - illumination_smoothness_loss: 0.1880 - color_constancy_loss: 0.6419 - exposure_loss: 0.6848 - val_total_loss: 1.4724 - val_illumination_smoothness_loss: 0.2328 - val_color_constancy_loss: 0.7439 - val_exposure_loss: 0.4957\n",
            "Epoch 46/50\n",
            "12/12 [==============================] - 7s 604ms/step - total_loss: 1.5060 - illumination_smoothness_loss: 0.1817 - color_constancy_loss: 0.6409 - exposure_loss: 0.6833 - val_total_loss: 1.4665 - val_illumination_smoothness_loss: 0.2254 - val_color_constancy_loss: 0.7462 - val_exposure_loss: 0.4949\n",
            "Epoch 47/50\n",
            "12/12 [==============================] - 8s 601ms/step - total_loss: 1.4982 - illumination_smoothness_loss: 0.1755 - color_constancy_loss: 0.6404 - exposure_loss: 0.6823 - val_total_loss: 1.4604 - val_illumination_smoothness_loss: 0.2191 - val_color_constancy_loss: 0.7484 - val_exposure_loss: 0.4930\n",
            "Epoch 48/50\n",
            "12/12 [==============================] - 7s 605ms/step - total_loss: 1.4920 - illumination_smoothness_loss: 0.1701 - color_constancy_loss: 0.6404 - exposure_loss: 0.6815 - val_total_loss: 1.4553 - val_illumination_smoothness_loss: 0.2135 - val_color_constancy_loss: 0.7497 - val_exposure_loss: 0.4921\n",
            "Epoch 49/50\n",
            "12/12 [==============================] - 8s 666ms/step - total_loss: 1.4864 - illumination_smoothness_loss: 0.1657 - color_constancy_loss: 0.6406 - exposure_loss: 0.6800 - val_total_loss: 1.4498 - val_illumination_smoothness_loss: 0.2082 - val_color_constancy_loss: 0.7509 - val_exposure_loss: 0.4907\n",
            "Epoch 50/50\n",
            "12/12 [==============================] - 7s 602ms/step - total_loss: 1.4811 - illumination_smoothness_loss: 0.1613 - color_constancy_loss: 0.6409 - exposure_loss: 0.6789 - val_total_loss: 1.4447 - val_illumination_smoothness_loss: 0.2028 - val_color_constancy_loss: 0.7526 - val_exposure_loss: 0.4893\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b3a1ed55750>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "Image_Enhancer = ZeroDCE()\n",
        "Image_Enhancer.compile(learning_rate = 1e-4)\n",
        "Image_Enhancer.fit(train_dataset,validation_data=val_dataset,epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hzaH7US2rg-9"
      },
      "outputs": [],
      "source": [
        "def low_to_high_light(original_image, Image_Enhancer):\n",
        "    # Convert the input image to a NumPy array and normalize it to the range [0, 1]\n",
        "    image = tf.keras.utils.img_to_array(original_image)\n",
        "    image = image.astype(\"float32\") / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # Enhance the image using the provided Image_Enhancer model or function\n",
        "    output_image = Image_Enhancer(image)\n",
        "\n",
        "    # Convert the enhanced image back to uint8 and create a PIL Image\n",
        "    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n",
        "    output_image = Image.fromarray(output_image.numpy())\n",
        "\n",
        "    return output_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVMJLT8VwFh0"
      },
      "source": [
        "Displaying Results for 10 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W7Wz9jKDrg-9"
      },
      "outputs": [],
      "source": [
        "psnr_ratio = []\n",
        "for i in range(len(test_low_light_image_paths)):\n",
        "    # Load low-light image and enhance\n",
        "    low_light_image = Image.open(test_low_light_image_paths[i])\n",
        "    enhanced_image = low_to_high_light(low_light_image, Image_Enhancer)\n",
        "\n",
        "    # Load corresponding high-light image\n",
        "    high_light_image = Image.open(test_high_light_image_paths[i])\n",
        "\n",
        "    # Calculate PSNR between high-light and enhanced images\n",
        "    psnr = calculate_psnr(high_light_image, enhanced_image)\n",
        "    psnr_ratio.append(psnr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7THYKlRvCBI",
        "outputId": "9b82cf28-f097-4010-cd48-946c88cf7907"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.00094429035725"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "np.average(psnr_ratio)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}